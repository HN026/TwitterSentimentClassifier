{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f55a6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bf932a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49b1c1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (0.15.2)\n",
      "Requirement already satisfied: torchaudio in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: filelock in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (4.6.3)\n",
      "Requirement already satisfied: sympy in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.8.0)\n",
      "Requirement already satisfied: wheel in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: cmake in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.27.2)\n",
      "Requirement already satisfied: lit in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Requirement already satisfied: numpy in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torchvision) (1.25.0)\n",
      "Requirement already satisfied: requests in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/huzaifa/miniconda3/envs/tensorflow/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20393508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to:  The book was awesome\n",
      "----------------------------------------\n",
      "1 The movie was great\n",
      "2 Just finished reading 'Embeddings in NLP'\n",
      "3 I just ordered fried chicken üê£\n",
      "4 What time is the next game?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from collections import defaultdict\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModel.from_pretrained(MODEL)\n",
    "\n",
    "def get_embedding(text):\n",
    "  text = preprocess(text)\n",
    "  encoded_input = tokenizer(text, return_tensors='pt')\n",
    "  features = model(**encoded_input)\n",
    "  features = features[0].detach().cpu().numpy() \n",
    "  features_mean = np.mean(features[0], axis=0) \n",
    "  return features_mean\n",
    "\n",
    "query = \"The book was awesome\"\n",
    "\n",
    "tweets = [\"I just ordered fried chicken üê£\", \n",
    "          \"The movie was great\", \n",
    "          \"What time is the next game?\", \n",
    "          \"Just finished reading 'Embeddings in NLP'\"]\n",
    "\n",
    "d = defaultdict(int)\n",
    "for tweet in tweets:\n",
    "  sim = 1-cosine(get_embedding(query),get_embedding(tweet))\n",
    "  d[tweet] = sim\n",
    "\n",
    "print('Most similar to: ',query)\n",
    "print('----------------------------------------')\n",
    "for idx,x in enumerate(sorted(d.items(), key=lambda x:x[1], reverse=True)):\n",
    "  print(idx+1,x[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "497a7961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature extraction:\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "import numpy as np\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
    "text = \"Good night üòä\"\n",
    "text = preprocess(text)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "#Pytorch\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "model = AutoModel.from_pretrained(MODEL)\n",
    "features = model(**encoded_input)\n",
    "features = features[0].detach().cpu().numpy()\n",
    "features_mean = np.mean(features[0], axis=0)\n",
    "\n",
    "features_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "869cf3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "I am so <mask> üòä\n",
      "1) happy0.402\n",
      "2) excited0.1441\n",
      "3) proud0.143\n",
      "4) grateful0.0669\n",
      "5) blessed0.0334\n",
      "------------------------------\n",
      "I am so <mask> üò¢\n",
      "1) sad0.2641\n",
      "2) sorry0.1605\n",
      "3) tired0.138\n",
      "4) sick0.0278\n",
      "5) hungry0.0232\n"
     ]
    }
   ],
   "source": [
    "#Masked Language Modeling: \n",
    "\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
    "fill_mask = pipeline(\"fill-mask\", model=MODEL, tokenizer=MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def print_candidates():\n",
    "    for i in range(5):\n",
    "        token = tokenizer.decode(candidates[i]['token'])\n",
    "        score = np.round(candidates[i]['score'], 4)\n",
    "        print(f\"{i+1}){token}{score}\")\n",
    "\n",
    "texts = [\n",
    "    \"I am so <mask> üòä\",\n",
    "    \"I am so <mask> üò¢\"\n",
    "]\n",
    "\n",
    "for text in texts: \n",
    "    t = preprocess(text)\n",
    "    print(f\"{'-'*30}\\n{t}\")\n",
    "    candidates = fill_mask(t)\n",
    "    print_candidates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b3ea3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a700271f8257413f9705dadba1e7366b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/768 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcea2643ada4461bc88ea84d0a75de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95a4023a83c406aabfc495c4246addc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f831424de844d98657b05ae9941b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "task='emotion'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f4382b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD LABEL MAPPING\n",
    "\n",
    "mapping_link = f\"https://github.com/HN026/TwitterSentimentClassifier/tree/master/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fc270a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255f8021f9784cddaeb9feba1246bd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "text = \"Good night üòä\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model (**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a976968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
